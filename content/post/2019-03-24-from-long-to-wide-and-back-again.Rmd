---
title: From Long to Wide and Back Again
author: Recle E. Vibal
date: '2019-03-24'
slug: long-wide-back-again-cdata
categories:
  - data manipulation
  - data wrangling
tags:
  - cdata
  - data.table
description: 'Adventures in Data Transformation and an Appreciation of {cdata}'
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
topics: []
---

```{r packages-options, message=FALSE, warning=FALSE, include=TRUE}

# Attach packages and set chunk options

xfun::pkg_attach(
  "knitr",
  "DT",
  "data.table",
  "cdata",
  "tidyr",
  "tibble"
)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = FALSE
)

```

# Inevitable

With my current work as a [junior data scientist](https://amateurdatasci.rbind.io/post/loves-what-he-is-doing/), I have to manipulate a lot of data to get to a desired output or a possible answer to a question. Almost all data that I have handled required me to convert a wide table to a long format or a long table to a wide format.

The usual packages I use for length and cross wise transformations are `{tidyr}`[@R-tidyr] and `{data.table}`[@R-data.table]. For wide to long^[I find going from wide to long much easier and more intuitive than going from long to wide, but that might be because I am more exposed and have more experience converting from wide to long as this is the more common data transformation that I have encountered so far.], one can use either `tidyr::gather()` or `data.table::melt()`; for long to wide, one can use `tidyr::spread()`^[I just heard about `{tidyr}`'s [`pivot_long()` and `pivot_wide()`](https://tidyr.tidyverse.org/dev/articles/pivot.html) as possible replacements for `gather()` and `spread()`. The vignette mentions `{cdata}` and `{data.table}`'s `melt()` and `dcast()` as inspiration for the improvements. I am excited to give `pivot_long()` and `pivot_wide()` a spin, but I my preference will stay with `{cdata}`.] and `data.table::dcast()`^[I must confess that `dcast()` is my weak point in using `{data.table}`. I felt its power during the times I have used it successfully, but I somehow have to spend time reading the documentation and do test transformation everytime I try to use it. Again, most of my experience involves transforming data from wide to long. Maybe I would have mastered `dcast()` better with more practice.].

The functions from both packages have the similar disadvantage of their use cases not really being intuitive in terms of how to transform the data. In my opinion, data is transformed in the mind of the user, i.e. the transformation is hidden within the code that the future user and collaborators have to decipher. It will be better if the user can work with the data and just allow the code to obey its master. I think this concern is addressed by `{cdata}`.

# Coordinatized

While I advocate the use and am a heavy user of `{cdata}`, the best resource to learn it is from its [vignettes](https://winvector.github.io/cdata/index.html). The main concept behind `{cdata}` is the theory of *coordinatized*^[Learn more about coordinatized data [here](http://www.win-vector.com/blog/2017/03/coordinatized-data-a-fluid-data-specification/) and [here](http://winvector.github.io/FluidData/FluidData.html)] or *fluid* data, each element of the data can be located using coordinates defined by the table name, key column/s and value/s, and the value column.

I really like the idea of coordinatized data because all data is coordinatized. We do not need to identify which data is "tidy" or "messy". Data is data; and the form a data has in the beginning is not important as long as we know how to interpret it in terms of its coordinates. Once we understand the data, we can break it down and shape it into the form that will be most useful for our purposes.

`{cdata}` is also the first `R` packacge that talked about row records and blocks which may seem confusing at first (at least, for me) but it makes more sense to think about data transformations from row records to blocks, and vice versa. Long and wide might be simpler and maybe even correct when a set of columns are placed into two or when two columns are distributed into multiple columns, but for more complicated transformations, I beleive, row records and blocks are more appropriate. 

How does `{cdata}` perform fluid data transformation? How is it different from `{tidyr}`'s `gather()` and `spread()` or `{data.table}`'s `melt()` and `dcast()`?

# One Data to Compare Them All

I will compare data transformation using `{tidyr}`, `{data.table}`, and `{cdata}`, and I hope I will higlight the power of `{cdata}`. I will use the Lord of the Rings data from [Hiroaki Yutani's blog post](https://yutani.rbind.io/post/enhancing-gather-and-spread-by-using-bundled-data-frames/)^[Observe that my reply included a suggestion to study `{cdata}`. I changed the data a little. The first book of The Lord of the Rings (a Tolkien fan here) is The Fellowship of the Ring, so I think FoTR will be a better column name.].

```{r prepare_lotr}

lotr_tbl <- tribble(
  ~Race   , ~Female_FoTR, ~Male_FoTR, 
            ~Female_TT  , ~Male_TT  ,
            ~Female_RoTK, ~Male_RoTK,
  "Elf"   , 1229        , 971       , 
            331         , 513       ,
            183         , 510       ,
  "Hobbit", 14          , 3644      , 
            0           , 2463      ,
            2           , 2673      ,
  "Man"   , 0           , 1995      ,
            401         , 3589      ,
            268         , 2459
)

lotr_dt <- setDT(lotr_tbl)

```

My first goal is to convert the data from this (Table 1)

```{r show-lotr, fig.cap="Table 1"}

head(lotr_tbl)

```

to this (Table 2)

```{r show-lotr-transformed}

lotr_tbl_2 <- tribble(
  ~Race   ,	~key    ,	~FoTR, ~TT,	~RoTK,
  "Elf"   ,	"Female",	1229 , 331 , 183 ,
  "Hobbit",	"Female",	14   , 0	 , 2   ,
  "Man"	  , "Female",	0    , 401 , 268 ,
  "Elf"   ,	"Male"  ,	971  , 513 , 510 ,
  "Hobbit",	"Male"  ,	3644 , 2463, 2673,
  "Man"   ,	"Male"	, 1995 , 3589, 2459
)

lotr_tbl_2

```

And then back to Table 1.

## Gather and Spread {#tidyr-spread-gather}

To convert from Table 1 to 2, a single use of `{tidyr}`'s `gather()` will not work because we want to take multiple columns and put it in multiple columns rather than only two. We will need the full functionality of `{tidyr}`.

My solution to convert Table 1 to 2 is

```{r tidyr-1-to-2}

lotr_tbl %>% 
  gather(key = "key", value = "count", - Race) %>% 
  separate(key, into = c("key", "book"), sep = "_") %>% 
  spread(key = book, value = count)

```

I collected all columns first, except the `Race` column into two colums. The key column is a combination of the gender and book. Since we want book to be column headers, we need to separate this single key column into two (`key` and `book`) using `tidyr::separate()`. With the `book` only in one column, we can spread this column and get to Table 2.

For Table 2 to 1

```{r tidyr-2-to-1}

lotr_2 %>% 
  gather(key = "book", value = "count", -Race, -key) %>% 
  unite(col = "key", key, book, sep = "_") %>% 
  spread(key = key, value = count)
  

```

I gatherd all the book/count columns and put it in two columns. I used `tidyr::unite()` to combine `key` and `book`. I finished the transformation with a `spread()` of the `key` and `count` column^[I offered a different soultion in Hiroaki's blog that also used `tidyr::nest()` and `tidyr::unnest()`, but that was only for comparison with the suggested `bundle()` function. The solution I offer here with `separate()` and `unite()` might be more intuitive and simpler.].

## Melt and (D)Cast

The steps we used in the previous section can also be translated in `{data.table}`'s `melt()` and `dcast()`.

```{r rdatatable-1-to-2}

lotr_dt[
  ,
  melt(.SD, id.vars = "Race", value.name = "count")
][
  ,
  tstrsplit(variable, split = "_", names = c("key", "book")),
  .(Race, count)
][
  ,
  dcast(.SD, Race + key ~ book, value.var = "count")
]

```

Note that I only specified `id.vars` since this is the only column that will not be collected. In more complicated cases, I think it will be necessary to specify both `id.vars` and `measure.vars` as a vector of column names. Using `melt()` with `.SD`^[I like how much of `{data.table}`'s functionality happens within `[`. I find functions that force me to get out of that "box" hindering my workflow. As a personal remedy, I use functions with `.SD` at some loss in efficiency. It is my personal wish that `.SD` becomes more efficient as I use it a lot.] is a matter of personal preference. The `data.table::tstrsplit()` helps in splitting one column into two. The `formula` argument in `dcast()` has always been confusing to me. I use `melt()` more often and still check its documentation and arguments. With `dcast()` it will require me more study and a bit of trial and error to make it work. In my limited understanding, The `formula` argument wants to know which id columns (`Race` and `key`) are a function of which value column. Specifying `value.var` in this example is not necessary because it is the only column left for casting, but it is better to be clear.

```{r rdatatable-2-to-1}

# data.table::copy() is necessary so that lotr_tbl_2 is not affected.
lotr_dt_2 <- setDT(copy(lotr_tbl_2))

lotr_dt_2[
  ,
  melt(
    .SD, id.vars = c("Race", "key"), 
    measure.vars = c("FoTR", "TT", "RoTK"),
    variable.name = "book", value.name = "count"
  )
][
  ,
  .(key = paste(key, book, sep = "_")),
  .(Race, count)
][
  ,
  dcast(.SD, Race ~ key, value.var = "count")
]

```

We can also convert from Table 2 to 1 using `{data.table}` by reversing our process. I do not know if `tstrsplit()` has a complementary function in `{data.table}`, but `paste()` seems to work really fine.

## Reshape

Since I am in full comparison mode, let us include a data transformation method using `stats::reshape()`.

```{r reshape-1-to-2}

key_book <- names(lotr_tbl)[names(lotr_tbl) != "Race"]

lotr_df <- 
  reshape(
    lotr_dt, 
    direction = "long",
    varying = list(key_book),
    idvar = "Race", timevar = "key",
    times = key_book, v.names = "count"
  )

lotr_df[, c("key", "book")] <- transpose(strsplit(lotr_df$key, split = "_"))

lotr_df_2 <- 
  reshape(
    lotr_df,
    direction = "wide", 
    timevar = "book", v.names = "count",
    idvar = c("Race", "key"), sep = ""
  )

names(lotr_df_2) <- gsub("count", "", names(lotr_df_2))

lotr_df_2

```

To be honest, this is the only opportunity that I have used `reshape()`, so this may not be the best data transformation method available in `R` without using external packages. The argument names are still confusing to me, but with more practice I think I can feel comfortable using it. However, I find that `varying` as a list, the need to specify `times` as a vector of column names, and the extra step to remove `v.names` string after the "widening" to be features that would need more getting used to.

Reversing the above process,

```{r reshape-2-to-1}

books <- names(lotr_df_2)[!names(lotr_df_2) %in% c("Race", "key")] 

lotr_df_1 <- 
  reshape(
    lotr_df_2,
    direction = "long", idvar = c("Race", "key"),
    varying = list(books), times = books,
    timevar = "book", v.names = "count"
  )

lotr_df_1 <- within(lotr_df_1, key_book <- paste(key, book, sep = "_"))

lotr_df_1 <- 
  reshape(
    lotr_df_1,
    direction = "wide", idvar = "Race",
    timevar = "key_book", v.names = "count",
    drop = c("key", "book"), sep = ""
  )

names(lotr_df_1) <- gsub("count", "", names(lotr_df_1))

lotr_df_1

```

I think I really like `reshape()` even with its quirks. I did not know `{stats}` has this powerful tool all along. Exploring available tools in `R` before looking in external packages provides a better understanding and awareness of `R`'s power.

## Rowrecs and Blocks

# Code {#code}

The following codes show what I did in this post. I added comments here and there to clarify my thought process.

```{r all-codes, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}

```

My session info for reproducibility.

```{r session-info}

sessioninfo::session_info(include_base = TRUE)

```

# References

For more information on tidy principles, techniques, and good practices, and the packages and functions I used in this post, please read