---
title: Wrangling and Manipulation of Monthly Philippine Consumer Price Index
author: Recle Etino Vibal
date: '2018-11-15'
categories:
  - data manipulation
  - data wrangling
  - economics
  - lessons
  - Philippines
tags:
  - baseR
  - consumer price index
  - data.table
  - rebus
slug: wrangling-manipulation-ph-cpi-data-table
description: Elongating a Wide Table Using {data.table} and {base} R
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
bibliography:
  - references/wide-long-data-table-base.bib
biblio-style: apalike
---

```{r packages-opts, message=FALSE, warning=FALSE, include=FALSE}

# Attach packages and set chunk options

xfun::pkg_attach(
  "knitr",
  "DT",
  "data.table",
  "wrapr"
)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = FALSE
)

# create standard datatable function to be used in all tables in this post

show_datatable <- function(cpi_data) {
  cpi_data %>% 
    DT::datatable(
      rownames = FALSE,
      extensions = 'FixedColumns',
      options = list(
        pageLength = 20,
        scrollX = TRUE,
        scrollCollapse = TRUE,
        columnDefs = list(
          list(
            className = 'dt-center',
            targets = 0:(ncol(cpi_data) - 1)
          )
        )
      )
    )
}

```

# More Paths, More Knowledge

In a series of [posts](https://amateurdatasci.rbind.io/post/messy-tidy-identify-split-geographic-scope/), I discussed how I turned a messy data, the [monthly Philippine consumer price index](http://openstat.psa.gov.ph/dataset/prices-and-related-indices) to tidy data, using packages and functions in the `tidyverse`. I learned how to use `R` faster via the `tidyverse`, and I agree that for anyone going into data science with little or no knowledge about coding, `tidyverse` offers one of the easiest ways to learn `R`.

However, as I became more familiar and more comfortable with `R`, I realized there is a larger ecosystem of packages and functions outside of `tidyverse`. With >13,000 packages in [CRAN](https://cran.r-project.org/web/packages/available_packages_by_date.html), redundancy among packages and functions cannot be avoided, but I do not think this is a bad thing. The variety of packages in `R` allows for different approaches and philosophies^[I just learned about [{cdata}](https://cran.r-project.org/web/packages/cdata/index.html), coordinatized data, and fluid data transformations. The deeper you go into `R` the more you learn. It's a multiverse out there.] towards programming, data, and data science. 

In this post, I do everything I did with the Philippine CPI data without the using `tidyverse`. This is not to say that `tidyverse` is insufficient. It has its weaknesses, but I think for the most common data science tasks, `tidyverse` covers a lot of user needs. However, this also does not mean that `tidyverse` is the only way or the most preferred approach. The `R` ecosystem may be as diverse^[Calling dibs on `diverseverse`.] as its users.

# Extension of data.frame

We can do most of what we want to do with data using `{base}` `R` only. Packages in CRAN are tools that allow current and future users to learn from the past and what past users have done or found. This is just progress, and I think this is the strength of `R` as an open source software for statistical computing and graphics. Working with only `{base}` `R` is possible, but it will be difficult and limiting, I think, for any level of use`R`. Again, while `{base}` `R` may be sufficient for all of data science tasks for anyone who is very familiar with it, packages offer work arounds to common tasks.

After deciding to use a package, the next question for the use`R` is what package to use. For data manipulation, the most common go-to package is `dplyr`. However, this is not the only package that allows us to manipulate data, another package is `{data.table}`[@R-data.table], the extension of `data.frame`.

I really like `{data.table}`. It has a very different API compared to `dplyr`. It has its own advantages and disadvantages^[Advantages and disadvantages are subjective. I once thought that the difficulty of learning `{data.table}` when I already know `dplyr`. Working with multitude types and sizes of data "forced" me to try `{data.table}`. After that, I regretted passing on `{data.table}` and not learning it sooner. I think the disadvantages are only an illusion whenever learning something new, while the advantages are not really that clear until one really experiences what one is missing out.], but I think it is one of the best packages out there when it comes to data manipulation. 

I will repeat my process of transforming the Ph CPI data from the [Philippine Statistics Authority](http://openstat.psa.gov.ph/) using `{data.table}`^[I am not an expert use`R` of `{data.table}`. I am still learning and getting familiar with the functions and nuances of `{data.table}`. I expect that I will use it more in the future, so I think this is good practice for me. The code I will use here may not be the most efficient, and I will always be open for improvements.] as my main data manipulation tool.

# Where | Order By, Select | Update, Group By

I don't want this to be a tutorial about `{data.table}`. The [best tutorial](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) for `{data.table}` already exists. I will emphasize on the `{data.table}` syntax and explain why it is an extension of the `data.frame`.

With `{base}` `R`, we can subset or extract values from a `data.frame` object, `df`, using `df[rows, columns]` where `rows` is a vector of row indices or names and `columns` is a vector of column indices or names. In `{data.table}`, we can subset or extract values from a `{data.table}` object, `dt`, using `dt[i, j, by]`. `i` is a set of conditions to filter rows or columns we want to arrange the data with `order()`. `j` is a list of data transformations or summaries we want to do at specific columns. `by` is a list of columns we want the data transformations to be grouped. If the reader is familiar with [Structured Query Language](https://en.wikipedia.org/wiki/SQL) or SQL, `i` is equivalent to SQL's `where` and `order by`, `j` to `select` and `update`, and `by` to `group by`. Again, please refer to ["Introduction to data.table"](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) for more information.

I prefer to use vertical `{data.table}` syntax.

```

dt[
  i,
  j,
  by
]

```
This vertical coding helps me review my code vertically too. I just find this way easier for me.

# Monthly Philippine Consumer Price Index 1994 Jan to 2018 Jun

The things I did with the Ph CPI data are already discussed in my [previous posts](https://amateurdatasci.rbind.io/tags/consumer-price-index/). I will only translate them here in `{data.table}` syntax, so I will not repeat my reasons and thought processes. However, I will leave some comments on the functions I used. For the complete code, please refer to \@ref(code).

## Finagler

We read the raw Ph CPI data using `data.table::fread()`. In the documentation, `data.table::fread()` is a fast and freindly file finagler^[I do not know what finagler means. Merriam-Webster defines [finagle](https://www.merriam-webster.com/dictionary/finagle) as to obtain by indirect or involved means. I think `data.table::fread()` does that rather than obtain by trickery, although that sounds cooler.]. The output is an object of `data.table` class.

(ref:raw-data-fread) First 40 Rows of Monthly Philippine Consumer Price Index from 1994 Jan to 2018 Jun Imported Using data.table::fread().

```{r read-show-raw-data, fig.cap='(ref:raw-data-fread)'}

# Read Data ---------------------------------------------------------------

raw_ph_cpi <- 
  fread(
    "./data/ph-cpi-data/raw/CPI_2006=100_1994-2018_1_2.csv",
    # check.names = TRUE is necessary to fix the duplicate column names
    check.names = TRUE
  )

# Clean Columns -----------------------------------------------------------

# Remove the average columns and the extra V324 column

remove_columns <- 
  colnames(raw_ph_cpi)[
    grepl(
      "Ave_|V324",
      colnames(raw_ph_cpi)
    )
    ]

raw_ph_cpi[, (remove_columns) := NULL]

raw_ph_cpi %>.%
  head(., 40) %>.%
  show_datatable(.)

```

Note that I am also using `%>.%` from `wrapr` [@R-wrapr]^[I will probably explore `wrapr` more in the future.] for my pipes. It is just a strict version of `%>%` from `magrittr`. It is stricter because it does not allow the user to leave the first argument of the function after the pipe empty, i.e. it requires the first argument to be specified as `.`, otherwise it throws an error that the first argument is missing. Again, I am only doing this because I want to try functions outside of the `tidyverse`.

# ISO 8601 Date

To gather all the date columns into one, I used `data.table::melt()`. I also used `data.table:::patterns()` to specify the columns I want to gather. Converting the dates into [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html) format (YYYY-MM-DD) is a bit trickier. First, I needed to separate the `month.abb_yy` to `month` and `year`. I did this using `data.table::tstrsplit()`. Second, I made a string YYYY-MM-DD using the `month` and `year` plus specifying the day as 1. I wanted to use the last day of the month, but I could not find a function outside the `tidyverse` that will allow me to do this.

(ref:iso-8601-cpi) First 40 Rows of Monthly Philippine Consumer Price Index from 1994 Jan to 2018 Jun Converted to Long Form Using data.table::melt() and with ISO 8601 Dates.

```{r melt-cpi-one-date, fig.cap='(ref:iso-8601-cpi)'}

# Gather All the Date Columns into One ------------------------------------

one_date_column_cpi_ph <- 
  melt(
    raw_ph_cpi,
    measure.vars = patterns(
      paste(
        paste0(month.abb, "_"), 
        collapse = "|"
      )
    ),
    variable.name = "date",
    value.name = "cpi"
  )

# Convert Date Column to Date ---------------------------------------------

one_date_column_cpi_ph[
  ,
  # split date column into month and year colum using data.table::tstrsplit()
  c("month", "year") :=
    tstrsplit(date, "_")
][
  ,
  # convert character year to integer
  year := as.integer(year)
][
  ,
  # create a character date column
  date := paste(
    year,
    month,
    1,
    sep = "-"
  )
][
  ,
  # create a date in ISO 8601 format
  date_iso8601 := as.Date(
    date,
    format = "%y-%b-%d"
  )
][
  ,
  c(
    "Description",
    "cpi"
  ) := .(
    # replace Non-Alcholic to Non-Alcoholic
    gsub(
      "Non-Alcholic",
      "Non-Alcoholic",
      Description
    ),
    # change character CPI to numeric
    as.numeric(cpi)
  )
]

one_date_column_cpi_ph %>.%
  head(., 40) %>.%
  show_datatable(.)

```

# Item Category Levels

`data.table::tstrsplit()` becomes useful again in creating item codes by separating the `Code` column. Filtering and subsetting is really quick in `[data.table]`. It requires some getting used to, but once you get familiar with the syntax, the thinking and coding becomes quick and easy too. Combining `data.table`s using `merge()` was also very easy and intuitive.

(ref:categorized-items-cpi) First 40 Rows of Monthly Philippine Consumer Price Index from 1994 Jan to 2018 Jun Categorized per Item Level.

```{r item-levels-split-merge}

# Split Item Code Column into Three Item Category Columns -----------------

item_category_levels <- 
  one_date_column_cpi_ph[
    ,
    # create three new columns from the Code column
    c(
      "primary_item_code",
      "secondary_item_code",
      "tertiary_item_code"
    ) :=
      tstrsplit(
        Code,
        # using rebus::DOT for readable regex
        split = rebus::DOT
      )
  ]

# Create Item Levels for Merging Later ------------------------------------

primary_items <- 
  item_category_levels[
    # remove items with NA secondary and tertiary item code to keep only the
    # primary items
    is.na(secondary_item_code) &
      is.na(tertiary_item_code),
    .(
      # select only the unique item descriptions
      primary_item_label = unique(Description)
    ),
    # make sure to group by item code to get the one-to-one description to item
    # code relation
    primary_item_code
  ]

secondary_items <- 
  item_category_levels[
    # select only the items with existing secondary item codes and NA tertiary
    # item codes to select only the secondary items
    !is.na(secondary_item_code) &
      is.na(tertiary_item_code),
    .(
      # select only the unique item descriptions
      secondary_item_label = unique(Description)
    ),
    .(
      # make sure to group by primary and secondary item code to get the
      # one-to-one description to item code relation for secondary items and
      # their accompanying primary labels
      primary_item_code,
      secondary_item_code
    )
    ]

tertiary_items <- 
  item_category_levels[
    # if tertiary item code is NOT NA, then it is a tertiary item
    !is.na(tertiary_item_code),
    .(
      # select only the unique item descriptions
      tertiary_item_label = unique(Description)
    ),
    .(
      # make sure to group by primary, secondary, and tertiary item code to get
      # the one-to-one description to item code relation for tertiary items and
      # their accompanying primary and secondary labels
      primary_item_code,
      secondary_item_code,
      tertiary_item_code
    )
    ]

# Add Category Labels to Main Table ---------------------------------------

categorized_cpi_data <- 
  item_category_levels[
    ,
    # no need for Description column after adding the item labels
    Description := NULL
  ] %>.%
  merge(
    .,
    primary_items,
    by = "primary_item_code",
    all = TRUE
  ) %>.%
  merge(
    .,
    secondary_items,
    by = c(
      "primary_item_code", 
      "secondary_item_code"
    ),
    all = TRUE
  ) %>.%
  merge(
    .,
    tertiary_items,
    by = c(
      "primary_item_code", 
      "secondary_item_code", 
      "tertiary_item_code"
    ),
    all = TRUE
  )

categorized_cpi_data %>.%
  head(., 40) %>.%
  show_datatable(.)

```

# Geographic Levels

The way I handled the `Geographic.code`, `Geographic.code.1`, and `Geographic.Location` of the primary items CPI data with `{data.table}` is similar with the item categories: split and find the unique geographic codes and labels, then add the labels to the data with `merge()`.

(ref:geography-levels-primary-items) First 40 Rows of Monthly Philippine Consumer Price Index from 1994 Jan to 2018 Jun for Primary Items with Geographic Levels.

```{r primary-item-geographic-levels, fig.cap='(ref:geography-levels-primary-items)'}

# Select Only the Primary Items -------------------------------------------

primary_items_cpi <- 
  categorized_cpi_data[
    is.na(secondary_item_code) &
      is.na(tertiary_item_code)
  ][
    ,
    # doing this in a separate bracket because data.table does not allow
    # filtering and deleting in the same call
    c(
      "secondary_item_code",
      "tertiary_item_code",
      "secondary_item_label",
      "tertiary_item_label"
    ) := NULL
  ]

# Add Geography Levels/Labels ---------------------------------------------

geography_labeled_cpi <- 
  primary_items_cpi[
    ,
    geography_level :=
      ifelse(
        Geographic.code == 0 & Geographic.code.1 == 0,
        "national",
        ifelse(
          Geographic.code != 0 & Geographic.code.1 %in% c(0, 99),
          "regional",
          "provincial"
        )
      )
  ]

# create a tabble for regional labels

region_labels <- 
  geography_labeled_cpi[
    geography_level == "regional",
    .(
      region_label = unique(Geographic.Location)
    ),
    .(
      Geographic.code
    )
  ]

# Separate Table by Geography Level ---------------------------------------

national_primary_item_cpi <- 
  geography_labeled_cpi[
    geography_level == "national"
  ]

regional_primary_item_cpi <- 
  geography_labeled_cpi[
    geography_level == "regional"
  ]

provincial_primary_item_cpi <- 
  geography_labeled_cpi[
    geography_level == "provincial"
  ]

provincial_primary_item_cpi <- 
  merge(
    provincial_primary_item_cpi,
    region_labels,
    by = "Geographic.code",
    all = TRUE
  )

```

# The Power of Packages

`{data.table}` is a fun package to use. I am not yet familiar with all of its capabilities. I will use it more often^[I plan to use it more in future anyway as I expect to work with bigger and bigger data sets.], so I can learn its true potential.

Working with only a few packages and doing everything else with `{base}` `R` already accomplishes a lot of things. This encourages me to use as few packages as I can. However, I still think there are powerful packages and functions out there that people have already made, e.g. `{rebus}` [@R-rebus], that just makes data science easier. If I encountered a problem for at least [three times](https://twitter.com/drob/status/928447584712253440) and solve it using the same code, I will make a function or even a package to relieve my future self some trouble.

Packages were and are being created to solve specific and recurring problems. Solving problems that were already solvedis a good practice and test, but in the battleground, knowledge of what has worked before can save time and maybe a life. There are redunduncies among packages. Choose well. Choose wisely.

# Code {#code}

The following codes show what I did in this post. I added comments here and there to clarify my thought process.

```{r all-codes, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}

```

My session info for reproducibility.

```{r session-info}

sessioninfo::session_info()

```

# References

For more information on tidy principles, techniques, and good practices, and the packages and functions I used in this post, please read
